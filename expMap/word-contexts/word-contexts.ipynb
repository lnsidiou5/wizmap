{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import exists, join, basename\n",
    "from tqdm import tqdm\n",
    "from json import load, dump\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# import local wizmap\n",
    "path = Path.cwd().parent.parent / \"notebook_widget\" / \"wizmap\" / \"wizmap.py\"\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"wizmap\", path)\n",
    "wizmap = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(wizmap)\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1eb60",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a88f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"train.json\", \"r\") as f:\n",
    "    texts = load(f)\n",
    "texts_arr = np.array([texts[f\"{i}\"] for i in range(len(texts))])\n",
    "embs = np.loadtxt(\"12.txt\")\n",
    "\n",
    "# Lists to store each part\n",
    "array_indices = []\n",
    "word_indices = []\n",
    "words = []\n",
    "categories = []\n",
    "\n",
    "# Open and parse the file\n",
    "with open(\"train.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()  # remove newline\n",
    "        if not line:\n",
    "            continue  # skip empty lines\n",
    "        \n",
    "        # Split at ':' to separate indices from word+category\n",
    "        indices_part, rest = line.split(\":\", 1)\n",
    "        \n",
    "        # Remove parentheses and split indices\n",
    "        array_idx, word_idx = indices_part.strip(\"()\").split(\",\")\n",
    "        array_idx = int(array_idx)\n",
    "        word_idx = int(word_idx) - 1\n",
    "        \n",
    "        # Split word and category by whitespace\n",
    "        word, category = rest.strip().split()\n",
    "        \n",
    "        # Append to lists\n",
    "        array_indices.append(array_idx)\n",
    "        word_indices.append(word_idx)\n",
    "        words.append(word)\n",
    "        categories.append(category)\n",
    "\n",
    "# Optional: convert indices to numpy arrays\n",
    "array_indices = np.array(array_indices)\n",
    "word_indices = np.array(word_indices)\n",
    "words = np.array(words)\n",
    "categories = np.array(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a525cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4284, 768)\n",
      "(1859,)\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "print(embs.shape)\n",
    "print(texts_arr.shape)\n",
    "#print(text_arr[0:4])\n",
    "# print(array_indices[:5])\n",
    "# print(word_indices[:5])\n",
    "# print(words[:5])\n",
    "# print(categories[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f4cd13",
   "metadata": {},
   "source": [
    "# Dim Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d49d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = UMAP(metric=\"cosine\")\n",
    "embeddings_2d = reducer.fit_transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08847e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(f\"UMAP Projected Embeddings of {embs.shape[0]} Texts\")\n",
    "# plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=0.1, alpha=0.2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ee8d9",
   "metadata": {},
   "source": [
    "# Wizmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3045e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = embeddings_2d[:, 0].astype(float).tolist()\n",
    "ys = embeddings_2d[:, 1].astype(float).tolist()\n",
    "\n",
    "# set texts with bracket around focus word\n",
    "temp_texts = []\n",
    "\n",
    "for text_i, word, idx in zip(array_indices, words, word_indices):\n",
    "    tokens = texts_arr[text_i].split()\n",
    "\n",
    "    # Safety check\n",
    "    if 0 <= idx < len(tokens) and tokens[idx] == word:\n",
    "        tokens[idx] = f\"[{tokens[idx]}]\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Mismatch at index {idx}: expected '{word}', found '{tokens[idx] if idx < len(tokens) else None}'\"\n",
    "        )\n",
    "\n",
    "    temp_texts.append(\" \".join(tokens))\n",
    "\n",
    "texts = np.array(temp_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af681b2",
   "metadata": {},
   "source": [
    "## Grid dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = wizmap.generate_data_list(xs, ys, texts)\n",
    "# The following is where the values are computed\n",
    "instructions = \"\"\"You are a linguist analyzing word usage. Given a set of sentences,\n",
    "            each containing a focus word, your task is to analyze these\n",
    "            sentences to determine how these focus words are commonly\n",
    "            used. Consider the word's part of speech, surrounding words,\n",
    "            tone, subject, context, and meaning. Summarize the highly\n",
    "            common patterns in 50 words or fewer, then list three key\n",
    "            descriptors.\n",
    "            For each sentence, you will receive:\n",
    "            - The focus word.\n",
    "            - The sentence, with the focus word enclosed in [].\n",
    "            Please note that these focus words may differ. Rather than\n",
    "            explaining them individually, focus on their common usage.\n",
    "            Where relevant, include concrete examples in your summary to\n",
    "            illustrate these patterns.\n",
    "            Provide your response in the following JSON format:\n",
    "            {\"keywords\": [\"descriptor1\",\n",
    "            \"descriptor2\", \"descriptor3\"], \"summary\": \"textual summary\"}\"\"\"\n",
    "grid_dict = wizmap.generate_grid_dict(xs, ys, texts, instructions, client, \"Word Contexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4faed75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wizmap.save_json_files(data_list, grid_dict, output_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22bb3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = (\n",
    "    \"https://raw.githubusercontent.com/lnsidiou5/wizmap/refs/heads/main/expMap/data.ndjson\"\n",
    ")\n",
    "grid_url = (\n",
    "    \"https://raw.githubusercontent.com/lnsidiou5/wizmap/refs/heads/main/expMap/grid.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249aab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display wizmaps\n",
    "wizmap.visualize(data_url = data_url, grid_url = grid_url, height=700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wizmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
